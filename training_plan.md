## Strategy for Developing Edge-Optimized AI Planner Models for an AI Assistant
1. Introduction
   
   The development of sophisticated AI assistants capable of complex planning tasks, such as trip itinerary generation or meeting scheduling under constraints, represents a significant advancement in intelligent systems. However, deploying such capabilities on resource-constrained edge devices like mobile phones presents substantial challenges, primarily related to model size, computational efficiency, and the need for robust constraint satisfaction. Current large language models (LLMs) exhibit impressive reasoning and language understanding but often falter in multi-step planning, constraint adherence, and maintaining logical consistency, particularly in complex scenarios.1 Furthermore, their large size (often >70B parameters) makes them unsuitable for edge deployment.This report outlines a strategic approach for developing specialized AI planner model(s) for an AI assistant, specifically targeting deployment on edge devices. The core requirements are:

- The model(s) must be 8 billion parameters or smaller.
- They must handle complex planning tasks with diverse constraints (e.g., budget, time, preferences, availability).
Performance should strive to be comparable to larger models while operating efficiently on edge hardware.
Seamless integration within the broader AI assistant architecture is essential.
To achieve these goals, this strategy proposes a hybrid approach combining the strengths of smaller, reasoning-focused LLMs with symbolic planning techniques, optimized for edge deployment using frameworks like MLC LLM.5 It details the model architecture, data generation and training methodologies, evaluation framework, optimization techniques, and integration patterns necessary for successful development and deployment.2. Proposed AI Planner Model ArchitectureAchieving robust and constrained planning on edge devices necessitates moving beyond monolithic LLM approaches. We propose a hybrid architecture that leverages the strengths of both LLMs and symbolic methods, tailored for models under 8B parameters.2.1. Base Language Model Selection (<8B Parameters)The foundation of the planner will be a pre-trained LLM fine-tuned for planning and reasoning tasks. Given the â‰¤8B parameter constraint, several open-source candidates offer a strong starting point:
Llama 3.1/3.2 8B: Meta's Llama models benefit from strong general reasoning, instruction following, large community support, and increasing context windows (8K-128K tokens).6 While versatile, they might require significant fine-tuning for specialized planning.6 Llama 3.3 adds multilingual support.7 Fine-tuning Llama 3 8B specifically for planning tasks has shown promise, suggesting its potential can be unlocked with targeted training data.10
Qwen 2.5 7B: Developed by Alibaba, Qwen models excel in conversational AI and structured output generation (like JSON), which could be beneficial for producing structured plans.6 They also possess strong multilingual capabilities.6 Distilled versions of larger reasoning models based on Qwen (e.g., DeepSeek-R1-Distill-Qwen-7B) show good reasoning performance in smaller sizes.11
DeepSeek Coder/Instruct 7B: These models are specifically strong in reasoning and code generation, making them potentially well-suited for tasks requiring logical deduction and structured planning.6 However, they might be less conversational than Qwen or Llama.6
Phi-4-Mini (3.8B) / Phi-4 (14B - Requires Pruning/Distillation): Microsoft's Phi models are designed for high performance-to-size ratio, excelling in reasoning and coding on consumer hardware.7 Phi-4-Mini (3.8B) has shown strong reasoning capabilities when specifically trained using tailored recipes involving distilled Chain-of-Thought (CoT) data and preference optimization.12 Phi-4-reasoning (14B), while slightly over budget, demonstrates state-of-the-art reasoning for its size class on planning and algorithmic benchmarks after SFT and RL, outperforming much larger models.15 A distilled or pruned version could fit the 8B constraint.
Gemma 2 (9B - Requires Pruning/Distillation): Google's Gemma 2 models are lightweight and optimized for reasoning and instruction following, suitable for resource-limited devices.7 The 9B model would need optimization to meet the 8B limit.
Recommendation: Given the emphasis on complex planning and constraint satisfaction, a Phi-4-Mini (3.8B) model specifically fine-tuned for reasoning (similar to Phi-4-Mini-Reasoning 12) or a fine-tuned Llama 3.1/3.2 8B appears most promising. Phi-4 offers efficiency and strong reasoning potential in small models 12, while Llama 3 provides a balance of capabilities and extensive community support.6 The choice may depend on initial benchmarking results after fine-tuning on planning-specific data.2.2. Hybrid LLM-Symbolic ArchitectureRelying solely on an LLM, even a fine-tuned one, for complex planning with hard constraints is risky due to potential inconsistencies, hallucinations, and difficulties with long-horizon reasoning.1 A hybrid approach integrating the LLM with a symbolic planner offers a more robust solution.1Proposed Workflow:
Natural Language Understanding & Constraint Extraction (LLM): The fine-tuned LLM receives the user's request (e.g., "Plan a two-week trip to Europe on a $3000 budget, visiting Paris and Rome, prefer museums") and extracts the core goal, entities, and constraints (budget, duration, locations, preferences). It also handles ambiguity inherent in natural language.17
Symbolic Problem Formulation (LLM): The LLM translates the extracted information into a formal representation suitable for a symbolic planner. This could involve generating Planning Domain Definition Language (PDDL) schemas 1 or formulating the problem as a constraint satisfaction/optimization problem solvable by SMT solvers or dedicated planning algorithms.18 This step leverages the LLM's ability to bridge natural and formal languages.20 Generating multiple candidate formalizations can help capture ambiguity.17
Plan Generation (Symbolic Planner): A lightweight classical planner (or constraint solver) takes the formal problem definition and searches for a valid plan that satisfies all constraints. This component guarantees logical soundness and constraint adherence, overcoming a key LLM weakness.1 Examples include Fast Downward 23 or specialized solvers integrated via code generation.18
Plan Translation & Presentation (LLM): The symbolic plan (often a sequence of actions) is translated back into a user-friendly natural language format by the LLM, matching the required output style.35
Advantages:
Robustness: Symbolic planners guarantee plan validity and constraint satisfaction.1
Constraint Handling: Explicitly models and solves hard constraints that LLMs struggle with.26
Reduced Hallucination: Grounds the planning process in formal logic, mitigating LLM tendencies to invent invalid steps.1
Efficiency: Smaller LLMs can focus on translation tasks, while efficient symbolic solvers handle the complex search.
This hybrid architecture separates concerns: the LLM handles the flexible interpretation of natural language and translation, while the symbolic component ensures rigorous planning and constraint satisfaction.3. Training Data StrategyHigh-quality training data is paramount for developing a capable planner LLM, especially smaller models which require carefully designed data to develop robust reasoning capabilities.12 We propose a strategy combining existing benchmarks with targeted synthetic data generation.3.1. Leveraging Existing DatasetsWhile specialized planning datasets for LLMs are emerging, we can leverage existing resources:
NATURAL PLAN Benchmark: This benchmark 34 provides realistic planning tasks (Trip Planning, Meeting Planning, Calendar Scheduling) with natural language descriptions and constraints, grounded in real-world tool outputs (Google Flights/Maps/Calendar). It includes golden plans in a specific natural language format 35 and evaluation scripts.36 This dataset is ideal for both fine-tuning (using examples as training data) and evaluation.
Other Planning/Agent Datasets: Datasets from benchmarks like TravelPlanner 32, API-Bank 38, AgentBench 39, WebArena 40, or PDDL-based planning competition domains 23 can provide diverse planning scenarios and action sequences, although they might require reformatting into instruction-following pairs suitable for LLM training. Instruction tuning datasets like InstructDial 42, P3 43, Flan 43, or collections like Awesome-instruction-tuning 43 can provide general instruction-following capabilities but lack specific complex planning structures.
3.2. Synthetic Data GenerationGiven the scarcity of large-scale, high-quality, constrained planning datasets suitable for LLMs 46, synthetic data generation is crucial.12 Synthetic data allows us to control diversity, complexity, and the types of constraints covered.Proposed Pipeline:
Seed Problem Generation:

Start with seed examples from existing datasets 35 or manually crafted scenarios covering target tasks (trip, meeting planning).
Use a powerful teacher LLM 13 to generate variations of these problems, introducing diverse constraints.51 Techniques like SynthLLM's concept extraction and recombination can enhance diversity.48 Focus on generating problems with varying complexity levels.35
Ensure constraints are realistic and potentially conflicting, requiring trade-offs. Include numerical constraints (budget, duration) and logical/temporal constraints (availability, dependencies).51


Plan/Solution Generation:

For each synthetic problem, use the teacher LLM or a combination of the LLM and a symbolic solver (as per the hybrid architecture) to generate candidate plans.
Generate Chain-of-Thought (CoT) reasoning steps alongside the plan, explaining how constraints were considered and decisions were made.1 This is vital for training reasoning capabilities, especially in smaller models.12 Methodologies like SWiRL generate multi-step tool use and reasoning trajectories.53
Generate both "good" (optimal/valid) and "bad" (suboptimal/invalid) plans to create preference pairs for DPO training. Bad plans can be generated by prompting the teacher model for suboptimal solutions or by introducing deliberate constraint violations.


Data Formatting & Filtering:

SFT Data: Format problems and their corresponding high-quality CoT plans into instruction-response pairs (e.g., <s> Problem Description CoT + Plan </s>).42
DPO Data: Format problems with pairs of preferred (good) and rejected (bad) plans (e.g., {"prompt": " Problem", "chosen": "Good Plan </s>", "rejected": "Bad Plan </s>"}).55
Filtering: Apply quality filtering. Use heuristics, symbolic verifiers, or even another LLM judge to check plan validity, constraint satisfaction, and reasoning coherence.53 Filter out low-quality or trivial examples. Process filtering (judging intermediate steps) can be more effective than outcome filtering alone.54


Dataset Augmentation: Employ techniques like paraphrasing requests 58, generating alternative valid plans for the same problem 58, or using methods like Self-Instruct 43 to further diversify the instruction set.
This synthetic data, combined with real examples, will form the core training corpus, specifically designed to teach the target <8B model complex, constrained planning. The focus should be on generating diverse problems with complex constraints and high-quality, verifiable reasoning steps.4. Training and Evaluation StrategyA multi-stage training process combining Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) is recommended to imbue the base LLM with robust planning capabilities and align it with desired behaviors like constraint adherence and efficiency. Evaluation will be anchored by the NATURAL PLAN benchmark and supplemented with specific constraint satisfaction metrics.4.1. Training Methodology
Supervised Fine-Tuning (SFT):

Goal: Teach the base model the fundamental task of planning, including understanding instructions, generating plans in the desired format 35, and performing basic reasoning steps (CoT).
Data: Use the curated dataset combining real examples 35 and high-quality synthetic data.12 Focus on diverse planning scenarios and constraint types.
Process: Fine-tune the selected <8B base model (e.g., Phi-4-Mini, Llama 3.1 8B) using standard causal language modeling objectives on the formatted SFT data.12 Techniques like packing short examples can improve efficiency.12 SFT provides the initial capability boost for the planning domain.61


Direct Preference Optimization (DPO):

Goal: Refine the SFT model to prefer plans that are not only valid but also optimal (e.g., efficient, adhering strictly to complex constraints) and align better with implicit human preferences captured in the data. DPO directly optimizes for preference alignment without needing a separate reward model, simplifying the process compared to traditional RLHF.55
Data: Use a preference dataset containing tuples of (prompt, chosen_plan, rejected_plan).55 chosen_plan represents a better plan (more optimal, better constraint satisfaction) and rejected_plan a worse one. This data can be generated synthetically (as described in Section 3.2) or potentially derived from human feedback or reward model scores.
Process: Apply DPO training to the SFT model using the preference dataset. DPO adjusts the model's likelihood of generating preferred responses over rejected ones.55 This stage is crucial for handling nuanced trade-offs and improving constraint satisfaction beyond basic validity.12 Combining SFT and DPO often yields better results than either alone, especially for complex tasks.63 Consider unified or alternating training schemes 65 to mitigate catastrophic forgetting between SFT and DPO objectives.61


This SFT+DPO approach leverages the strengths of both methods: SFT for initial domain adaptation and format learning, and DPO for fine-grained preference alignment and constraint optimization.124.2. Evaluation FrameworkA comprehensive evaluation framework is needed to assess the planner's performance, feasibility, and efficiency.
Benchmark: The primary benchmark will be NATURAL PLAN.34 It provides realistic tasks (Trip, Meeting, Calendar planning) with constraints and a defined evaluation protocol.
Core Metric:

Exact Match (EM) Score: As defined in NATURAL PLAN, this measures if the generated plan 35 exactly matches the golden plan in all key aspects (dates, times, locations, sequence).35 This is a strict but objective measure of correctness for tasks with a single optimal solution.


Supporting Metrics:

Plan Validity/Executability: Check if the generated plan is logically sound and executable within the defined domain rules (e.g., are flight connections possible? are locations reachable within given times?). This can involve rule-based checks or using the symbolic planner component to validate the plan.1
Constraint Satisfaction Rate: Measure the percentage of generated plans that successfully satisfy all explicit constraints mentioned in the prompt (e.g., budget limits, time windows, required visits, preference adherence).31 This requires parsing the plan and comparing it against the input constraints.
Task Success Rate: For tasks where multiple valid plans might exist or where the goal is complex (e.g., "meet as many friends as possible"), define a task-specific success metric (e.g., percentage of goals achieved, number of friends met).32
Efficiency/Optimality: Where applicable, measure plan efficiency (e.g., total trip duration, plan length, resource utilization) and compare against optimal solutions if known.66


Methodology:

Complexity Analysis: Evaluate performance across different complexity levels within NATURAL PLAN (e.g., varying number of cities, people, days) to understand scaling limitations.34 Performance often drops significantly with increased complexity.34
Ablation Studies: Compare the performance of the full hybrid model against LLM-only planning, and evaluate the impact of SFT vs. SFT+DPO.
Generalization Testing: Evaluate on held-out NATURAL PLAN tasks or potentially other planning benchmarks (reformatted if necessary) to assess generalization.66
Self-Correction Evaluation: Test the model's ability to identify and correct flaws in its initial plans when prompted, as explored in NATURAL PLAN.35
Robustness Testing: Evaluate performance under slightly paraphrased instructions or noisy input data.58


This evaluation framework, centered on NATURAL PLAN but augmented with validity and constraint checks, provides a rigorous assessment of the planner's capabilities relevant to the project goals.5. Model Optimization for Edge DevicesDeploying the planner model on edge devices necessitates aggressive optimization to meet memory, latency, and power constraints.69 The MLC LLM (Machine Learning Compilation for Large Language Models) framework is specifically designed for universal, high-performance deployment of LLMs on diverse hardware, including mobile GPUs (iOS/Android) and web browsers.55.1. QuantizationQuantization is the primary technique for reducing model size and accelerating inference. MLC LLM provides robust support for various quantization modes.74
Recommended Technique: 4-bit quantization (e.g., q4f16_1 or q4f32_1) offers a strong balance between compression and performance retention.74 It can reduce model size significantly (potentially ~75% compared to FP16) making models like Llama 3 8B or Phi-4 feasible for mobile deployment.69 MLC LLM uses grouping quantization by default.75
Supported Modes in MLC LLM:

Weight-Only: q4f16_1, q4f32_1 (4-bit weights, FP16/FP32 activations), q3f16_1 75, q0f16/q0f32 (no weight quantization, FP16/FP32 activations), q4f16_awq.76
Weight-Activation (CUDA only): e4m3_e4m3_f16, e5m2_e5m2_f16 (FP8 quantization).76


Process: MLC LLM provides tools (mlc_llm convert_weight) to apply these quantization modes during the weight conversion process.74 The mlc-chat-config.json file specifies the desired quantization mode.75
5.2. Other Potential Optimizations (Consideration)While quantization is the main focus for MLC LLM edge deployment, other techniques could be explored, potentially requiring integration with or extensions to the MLC framework:
Pruning: Removing redundant weights or structures from the model. While not explicitly detailed as a core MLC LLM feature in the provided snippets, techniques exist for pruning LLMs.79
Knowledge Distillation: Training the smaller target model (<8B) to mimic the outputs or internal representations of a larger, more capable teacher model (e.g., GPT-4, Claude 3). This is particularly relevant for enhancing reasoning in smaller models.12 The synthetic data generation process using a teacher LLM already incorporates elements of distillation.
Architectural Choices: Selecting base models known for efficiency 7 contributes significantly to edge performance.74
5.3. MLC LLM Toolchain for DeploymentMLC LLM provides an end-to-end workflow for preparing and deploying models on edge devices 71:
Model Conversion & Quantization: Convert the fine-tuned model weights into MLC format and apply the chosen quantization (e.g., q4f16_1) using mlc_llm convert_weight.78 Generate the mlc-chat-config.json using mlc_llm gen_config, specifying the model, quantization, and conversation template.78
Model Compilation: Compile the model architecture for the specific target platform (e.g., Android with Vulkan/OpenCL, iOS with Metal, WebGPU for web) using mlc_llm compile.71 This generates an optimized model library (.so, .dylib, .wasm, .tar).71 MLC LLM leverages Apache TVM for backend-specific optimizations.71
Packaging: Use mlc_llm package to bundle the compiled model library and quantized weights into a distributable format for integration into the mobile application (iOS/Android).83 This command reads a mlc-package-config.json file specifying target platforms and models.83
Inference: Utilize the MLC LLM runtime engine (MLCEngine) via its native APIs 5 within the AI assistant application to load the packaged model and perform inference.5
MLC LLM's focus on compilation and optimization specifically targets efficient execution on diverse hardware, making it a suitable choice for deploying the â‰¤8B planner model on mobile devices.5 Performance will depend on the specific device hardware 70 and the chosen quantization level.6. Integration Strategy within AI AssistantSeamlessly integrating the specialized AI planner into the broader AI assistant architecture is crucial for a cohesive user experience. A modular design using a Plan-and-Execute pattern is recommended.6.1. Agent Architecture: Plan-and-ExecuteInstead of a single monolithic agent, a Plan-and-Execute architecture separates high-level planning from low-level execution 40:
Planner Agent: This component houses the fine-tuned, optimized â‰¤8B hybrid LLM-symbolic planner model. It receives the user's request, relevant context/state, and constraints. Its role is to decompose the complex task and generate a structured, high-level plan.8540
Executor Agent: This component takes the plan generated by the Planner and executes each step. It might involve:

Calling internal assistant functions or external APIs/tools (e.g., calendar API, flight search API, maps API).67
Interacting with the user for clarification or confirmation.
Updating the task state based on execution results.
The Executor could be another LLM (potentially smaller/simpler or even the same model instance if resources allow) or a more deterministic code module interpreting the plan.40


Advantages:
Modularity: Clear separation of concerns simplifies development and maintenance.88
Efficiency: The potentially larger/slower Planner is called less frequently than in reactive patterns like ReAct. The Executor can be faster or use cheaper models/logic.84
Robustness: Explicit planning forces consideration of the entire task, potentially leading to better overall strategies.85 Allows for plan validation before execution.
6.2. API Design and InteractionThe interaction between the main assistant orchestrator, the Planner, and the Executor should be managed via well-defined APIs.88
Planner API:

Input: User request (natural language), current conversation context/history, relevant user profile information (preferences, past trips/meetings), explicit constraints (budget, dates, etc.), current world state (e.g., calendar availability). Constraints need to be clearly passed, potentially structured (JSON) or embedded in the prompt.32
Output: Structured plan 85, status (success/failure), confidence score (optional). The plan should be machine-interpretable by the Executor.


Executor API:

Input: A specific step from the plan, current state, necessary context/data.84
Output: Execution result (e.g., API response, data retrieved, confirmation), updated state, status (success/failure/needs_clarification).


Best practices include using clear interfaces, managing authentication/API keys securely 90, implementing logging and monitoring 88, and potentially using an API Gateway for managing calls if the architecture becomes complex.886.3. State ManagementEffective state management is critical for multi-step planning, context retention, and error recovery.94
Short-Term Memory: Maintain the context of the current planning session, including the user request, the generated plan, the status of each step, intermediate results, and conversation history.94 This can be managed by the orchestrator or within a dedicated memory module.95
Long-Term Memory: Store relevant user preferences, past plans, successful/failed outcomes, and learned constraints to personalize future interactions and improve planning over time.87 Vector databases 94 can store embeddings of past interactions for efficient retrieval. Knowledge graphs could model relationships between entities (e.g., user preferences, locations).94
Structured State Representation: Representing the task state hierarchically 96 can help track progress, dependencies between subtasks, and manage complex workflows involving parallel execution or rollbacks.85
State Updates: The Executor updates the state after each action. The Planner accesses the current state to generate or refine plans.94
6.4. Error Handling and ReplanningLLM agents are prone to errors, including generating invalid plans, failing to adhere to constraints, or encountering execution failures.67 A robust error handling and replanning strategy is essential.
Plan Validation: Before execution, validate the plan generated by the Planner for basic feasibility and constraint satisfaction using heuristics or the symbolic component.1
Execution Monitoring: The Executor should detect failures during action execution (e.g., API errors, constraint violations discovered at runtime).
Feedback Loop: Execution failures or validation errors should be fed back to the Planner.40
Replanning: Upon receiving feedback about errors or changes in the environment/constraints, the Planner should be invoked to revise the existing plan or generate a new one from the current state.85 Techniques like plan reflection 87 or prompting the LLM to analyze the failure and propose corrections can be used.98 Transactional approaches like SagaLLM could provide more robust failure recovery and rollback.99
This integrated approach ensures the planner operates effectively within the assistant, leveraging its specialized capabilities while maintaining context and handling inevitable errors gracefully.7. Conceptual Code ExamplesThese examples illustrate key concepts using Python pseudocode. Actual implementation would depend on the chosen LLM, frameworks (like LangChain, or custom), and the MLC LLM SDK/API.7.1. SFT Data PreparationPython# Assume 'raw_planning_data' contains tuples of
# (nl_request_with_constraints, structured_plan_or_nl_plan_with_cot)
# Example: ("Plan a 3-day trip to SF, budget $500", "Day 1: Arrive SFO...")

sft_dataset =
for request, plan_output in raw_planning_data:
  # Format for causal LM SFT, e.g., using Llama instruct format
  # Includes Chain-of-Thought (CoT) implicitly or explicitly in plan_output
  formatted_input = f"<s> Generate a detailed plan for the following request: {request} {plan_output} </s>"
  sft_dataset.append({"text": formatted_input})

# This sft_dataset can be used with Hugging Face Trainer or TRL SFTTrainer
# Reference: [12, 42, 45]
Rationale: This shows the basic formatting required to train the LLM to map natural language requests to plans (potentially including reasoning steps) using supervised learning.127.2. DPO Data PreparationPython# Assume 'preference_data' contains tuples of
# (nl_request, better_plan, worse_plan)
# Better/worse determined by constraint satisfaction, efficiency, etc.

dpo_dataset =
for request, chosen_plan, rejected_plan in preference_data:
  # Format for DPO training
  dpo_dataset.append({
      "prompt": f" Generate a detailed plan for the following request: {request}",
      "chosen": f"{chosen_plan} </s>", # Preferred completion
      "rejected": f"{rejected_plan} </s>" # Less preferred completion
  })

# This dpo_dataset can be used with TRL DPOTrainer
# Reference: [55, 56, 63]
Rationale: This illustrates how preference pairs (essential for DPO) are structured, enabling the model to learn finer-grained distinctions between good and bad plans based on constraints or other criteria.557.3. Planner Integration API Call (Conceptual)Pythonimport json
from typing import Dict, List, Any

# Assume 'planner_model' is an object wrapping the loaded MLC LLM engine
# planner_model = load_mlc_llm_planner("dist/planner_model_q4f16_1-MLC", "vulkan") # See 7.5

def invoke_planner(
    user_request: str,
    current_state: Dict[str, Any],
    constraints: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Calls the edge-deployed planner model via its API or SDK wrapper.
    Handles NL -> Formal translation implicitly or explicitly within the model.
    """
    try:
        # Construct a detailed prompt incorporating state and constraints
        prompt = f"""
        User Request: "{user_request}"

        Current State:
        {json.dumps(current_state, indent=2)}

        Constraints:
        {json.dumps(constraints, indent=2)}

        Task: Generate a step-by-step execution plan in JSON format that satisfies the request and constraints.
        Include actions, parameters, and expected outcomes.
        Example Action: {{"step": 1, "action": "search_flights", "parameters": {{"from": "SFO", "to": "CDG", "date": "2025-08-10"}}}}

        Generate the plan.
        """

        # Use MLC LLM engine for inference (blocking or streaming)
        # Assumes planner_model has a generate method
        raw_response = planner_model.generate(
            prompt,
            max_tokens=1024, # Adjust as needed
            temperature=0.1 # Low temperature for deterministic planning
        )

        # Parse the LLM response (expected to be JSON or parsable text)
        plan = json.loads(raw_response) # Add robust parsing and error handling

        # Optional: Validate plan structure or basic constraints here
        # validate_plan(plan, constraints)

        return {"plan": plan, "status": "success"}

    except Exception as e:
        print(f"Planner invocation failed: {e}")
        # Implement error handling/logging
        return {"plan": None, "status": "error", "message": str(e)}

# Reference: [32, 40, 90, 91, 95]
Rationale: This conceptual function demonstrates how the assistant would interact with the planner module, sending the request, current state, and constraints, and expecting a structured plan back. It highlights the importance of prompt engineering and structured output.907.4. Evaluation Metric (Constraint Satisfaction - Conceptual)Pythondef check_budget_constraint(plan: List[Dict[str, Any]], budget: float) -> bool:
    """Checks if the total estimated cost in the plan exceeds the budget."""
    total_cost = 0.0
    if not isinstance(plan, list):
        print("Warning: Plan is not a list, cannot check budget.")
        return False # Or handle appropriately

    for step in plan:
        if isinstance(step, dict) and "parameters" in step and isinstance(step["parameters"], dict):
            # Accumulate cost based on action type and parameters
            action = step.get("action", "").lower()
            params = step.get("parameters", {})
            if "cost" in params:
                 try:
                     total_cost += float(params["cost"])
                 except (ValueError, TypeError):
                     print(f"Warning: Invalid cost format in step: {step}")
            # Add logic for specific actions if cost isn't explicit
            # elif action == "book_flight":
            #     total_cost += estimate_flight_cost(params)
            # elif action == "book_hotel":
            #     total_cost += estimate_hotel_cost(params)
        else:
             print(f"Warning: Invalid step format: {step}")

    print(f"Total estimated cost: {total_cost}, Budget: {budget}")
    return total_cost <= budget

# --- Example Usage during Evaluation ---
# generated_plan_json = invoke_planner(...) # Get plan from model
# if generated_plan_json["status"] == "success":
#     generated_plan = generated_plan_json["plan"]
#     problem_constraints = get_constraints_for_problem(...) # Load ground truth constraints
#     budget = problem_constraints.get("budget")
#     if budget is not None:
#         is_satisfied = check_budget_constraint(generated_plan, float(budget))
#         print(f"Budget constraint satisfied: {is_satisfied}")
#     else:
#         print("No budget constraint specified for this problem.")

# Reference: [31, 35, 37, 51, 67]
Rationale: Provides a concrete example of how a specific evaluation metric (budget constraint satisfaction) could be implemented by parsing the structured plan output and comparing accumulated costs against the requirement.357.5. MLC LLM Inference (Conceptual Python API)Pythonfrom mlc_llm import MLCEngine
import json # For structured input/output

# --- Configuration ---
# Assume model weights/library are packaged using 'mlc_llm package' [83]
# Paths relative to where 'mlc_llm package' was run or absolute paths
model_id = "planner_model_q4f16_1" # Unique ID used during packaging
model_path = f"dist/bundle/{model_id}" # Contains params, config, tokenizer
# Library path depends on target - e.g., Android Vulkan
# Find the correct library name in dist/lib/ after packaging
model_lib_path = f"dist/lib/{model_id}-vulkan.so" # Example for Vulkan on Android
device = "vulkan" # Or "opencl", "cuda", "metal", "webgpu" depending on target [5]

# --- Load Model ---
print(f"Loading model {model_id} for device {device}...")
try:
    # Initialize the MLC Engine [71, 82, 108]
    engine = MLCEngine(
        model=model_path,
        model_lib=model_lib_path,
        device=device
    )
    print("Model loaded successfully.")
except Exception as e:
    print(f"Error loading model: {e}")
    exit()

# --- Prepare Prompt (using structure from 7.3) ---
user_request = "Plan a 2-week trip to Paris and Rome, budget $3000, focus on historical sites."
current_state = {"location": "San Francisco", "current_date": "2025-07-01"}
constraints = {"budget_usd": 3000, "duration_days": 14, "cities":, "interests": ["history"]}

prompt = f"""
User Request: "{user_request}"
Current State: {json.dumps(current_state)}
Constraints: {json.dumps(constraints)}
Task: Generate a step-by-step execution plan in JSON format.
 Generate the plan.
"""
print("\n--- Sending Prompt to Planner ---")
print(prompt)

# --- Generate Plan using MLC Engine Chat Completion API [71] ---
output_text = ""
print("\n--- Planner Response ---")
try:
    for response in engine.chat.completions.create(
        messages=[{"role": "user", "content": prompt}],
        model=model_id, # Use the model_id specified during packaging [83]
        max_tokens=1024, # Adjust based on expected plan length
        temperature=0.1, # Low temperature for more deterministic plans
        stream=True
    ):
        if response.choices:
            delta = response.choices.delta.content
            if delta:
                output_text += delta
                print(delta, end="", flush=True) # Print streamed output
    print("\n------------------------")

    # Attempt to parse the final JSON plan
    try:
        generated_plan = json.loads(output_text)
        print("\nParsed Plan (JSON):")
        print(json.dumps(generated_plan, indent=2))
        # Further processing/validation can happen here
    except json.JSONDecodeError:
        print("\nError: Failed to parse generated plan as JSON.")
        print("Raw Output:", output_text)

except Exception as e:
    print(f"\nError during generation: {e}")

# --- Unload Model ---
print("\nUnloading model...")
engine.unload()
print("Model unloaded.")

# Reference: [5, 71, 74, 78, 82, 83]
Rationale: This snippet demonstrates the end-to-end conceptual flow of using the MLC LLM Python API to load a pre-compiled and quantized model 78, prepare a prompt incorporating state and constraints, perform inference on the target device 5, stream the response, and attempt to parse the structured output.718. Conclusion and Future Directions8.1. Summary of Proposed StrategyThis report outlines a strategy for developing an AI planner model (â‰¤8B parameters) suitable for edge deployment within an AI assistant. The core recommendation is a hybrid LLM-symbolic architecture. This involves selecting a reasoning-focused base LLM under 8B parameters (e.g., a fine-tuned Phi-4-Mini or Llama 3.1/3.2 8B) 6 and integrating it with a symbolic planner/solver.1 The LLM handles natural language understanding, constraint extraction, and translation to/from a formal representation (like PDDL or optimization constraints) 17, while the symbolic component ensures plan validity and rigorous constraint satisfaction.1Training involves a multi-stage approach: Supervised Fine-Tuning (SFT) on a mix of real 35 and high-quality synthetic planning data 12 followed by Direct Preference Optimization (DPO) to align the model with complex constraints and efficiency goals.55 Synthetic data generation is crucial for covering diverse, constrained planning scenarios.48Evaluation will primarily use the NATURAL PLAN benchmark 34, focusing on Exact Match, plan validity, constraint satisfaction, and efficiency metrics across varying complexity levels.35For edge deployment, the MLC LLM framework 5 is recommended, utilizing 4-bit quantization 76 and MLC's compilation and packaging tools to optimize the model for mobile platforms (Android/iOS).74Integration within the assistant should follow a Plan-and-Execute pattern 40, with clear API definitions 90, robust state management 94, and effective error handling/replanning mechanisms.988.2. Feasibility and Trade-offsThis strategy presents a feasible path towards achieving robust, constrained planning on edge devices within the 8B parameter limit. The hybrid architecture directly addresses the known limitations of LLMs in rigorous planning and constraint handling.1 Using smaller, reasoning-optimized models like Phi-4 variants 12 combined with aggressive quantization via MLC LLM 74 makes edge deployment viable.However, trade-offs exist:
Complexity: Implementing and maintaining a hybrid system and a synthetic data pipeline is more complex than fine-tuning a single LLM.
Performance vs. Capability: While aiming for performance comparable to larger models, the inherent capacity limitations of <8B models mean there might be a ceiling on the complexity of planning problems they can handle effectively, even with symbolic assistance.12 Performance on benchmarks like NATURAL PLAN shows even large models struggle with high complexity.34
Optimization Impact: Quantization (especially 4-bit) can slightly degrade model performance, which needs careful evaluation.75
The investment in the hybrid architecture and synthetic data generation is justified by the need for reliable constraint satisfaction and planning validity, which are critical for a useful AI assistant and difficult to achieve with LLM-only approaches on the edge.8.3. Future DirectionsBuilding upon this strategy, several avenues for future research and development exist:
Advanced Synthetic Data: Develop more sophisticated methods for generating synthetic planning data that covers intricate constraint interactions, requires deeper reasoning, and better reflects real-world ambiguity and dynamism.46 Explore generating problems that specifically target known failure modes of LLM planners.
Refined Training Techniques: Investigate unified fine-tuning approaches that simultaneously optimize SFT and preference alignment objectives 61 to improve efficiency and mitigate catastrophic forgetting. Explore reinforcement learning techniques beyond DPO, such as RTO 103 or SWiRL 53, tailored for multi-step planning and tool use.
Enhanced Replanning/Error Recovery: Develop more sophisticated strategies for plan reflection, error diagnosis, and dynamic replanning when constraints change or execution fails.86 Explore techniques for learning from failures.
Symbolic Component Optimization: Optimize the chosen symbolic planner/solver itself for edge deployment, considering its own computational footprint.
Multi-Agent Planning: If the assistant requires coordinating multiple sub-tasks or interacting with other agents, explore extending the framework to multi-agent planning paradigms.32
Personalization and Continual Learning: Implement mechanisms for the planner to continuously learn and adapt based on user feedback, interaction history, and evolving preferences stored in long-term memory.94
By pursuing this strategy, focusing on the hybrid architecture, targeted training, rigorous evaluation, and edge optimization, it is possible to develop an effective and efficient AI planner that significantly enhances the capabilities of the AI assistant on mobile devices.